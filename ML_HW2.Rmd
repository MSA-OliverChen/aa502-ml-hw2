---
title: "Machine Learning HW2"
author: "Fall HW Team 6"
date: "2025-11-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prompt

* For this phase use only the insurance_t data set.
* Previous analysis has identified potential predictor variables related to the purchase of the insurance product so no initial variable selection before model building is necessary.
* The data has missing values that need to be imputed.
  * Typically, the Bank has used median and mode imputation for continuous and categorical variables but are open to other techniques if they are justified in the report.
* The Bank is interested in the value of random forest models.
  * Build a random forest model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. Make sure your target variable is a factor for the random forest.)
  * Tune the model parameters and recommend a final random forest model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
    * Pick one metric to rank things by – no need to report multiple metrics for each variable.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
* The Bank is also interested in the value of an XGBoost model.
  * Build an XGBoost model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. You will need to look up the documentation for the ‘objective = "binary:logistic" ‘ option.)
    * Use the area under the ROC curve (AUC) as your evaluation metric instead of the default in XGBoost.
  * Tune the model parameters and recommend a final XGBoost model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
    
# Setup

## Libraries

```{r}
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(DescTools)
library(progress)
```

## Data

```{r}
ins_t_raw = readr::read_csv('insurance_t.csv')
```

Imputation.

```{r}
ins_t = data.frame(ins_t_raw)
categoricals = c('DDA', 'DIRDEP', 'NSF', 'TELLER', 'SAV',
                 'ATM', 'CD', 'IRA', 'INV', 'MM', 'CC',
                 'SDB', 'INAREA', 'BRANCH')

ins_t = ins_t %>%
  mutate(across(categoricals,
                as.factor))

missing = ins_t %>%
  summarize(across(everything(), 
                   ~sum(is.na(.)))) %>%
  t() %>% 
  as.data.frame() %>%
  mutate(n_missing = V1) %>%
  dplyr::select(!V1) %>%
  filter(n_missing > 0)

missing_cols = missing %>% rownames()
# missing columns subsets for numeric and categorical data, respectively
missing_num = setdiff(missing_cols, categoricals)
missing_fctr = intersect(missing_cols, categoricals)

ins_t = ins_t %>%
  # Continuous variables: median imputation
  mutate(
    across(all_of(missing_num),
           ~ replace_na(.x, median(.x, na.rm = T))
    )
  ) %>%
  # Categorical variables: mode imputation
  mutate(
    across(all_of(missing_fctr),
           ~ replace_na(.x, Mode(.x, na.rm = T))
    )
  )
```


# Analysis

## Random Forest

## XGBoost

> Build an XGBoost model. (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. You will need to look up the documentation for the ‘objective = "binary:logistic" ‘ option.)
> Use the area under the ROC curve (AUC) as your evaluation metric instead of the default in XGBoost.

```{r}
ins_t_x = model.matrix(INS ~ ., data = ins_t)[, -1]
ins_t_y = ins_t$INS %>% as.numeric()

set.seed(123)
xgb_ins_1 = xgboost(
  data = ins_t_x,
  label = ins_t_y,
  subsample = 0.5, 
  nrounds = 100,
  # nfold = 5,
  objective = 'binary:logistic', 
  eval_metric = 'auc',
  maximize = T,
  verbose = 0
)
# I think these settings are overfitting
```

These default settings seem to be overfitting. Let's use cross-validation and tweak the hyperparameters (trying a few different values). 

> Tune the model parameters and recommend a final XGBoost model. 

```{r}
# Wrapper function to easily run cross validated
# binary classification XGBoost with AUC as the 
# objective function.
xgb_options = function(x, y, 
                       eta, 
                       max_depth, 
                       subsample, 
                       verbose = 1, 
                       seed = 123,
                       early_stopping_rounds = 10,
                       nfold = 5,
                       nrounds = 100,
                       min_child_weight = 1) {
  set.seed(seed)
  xgb = xgb.cv(
    # settings
    data = x,
    label = y,
    objective = 'binary:logistic', 
    eval_metric = 'auc',
    maximize = T,
    verbose = verbose,
    early_stopping_rounds = early_stopping_rounds,
    
    # hyperparameters
    nfold = nfold,
    eta = eta,
    subsample = subsample, 
    max_depth = max_depth,
    nrounds = nrounds,
    min_child_weight = min_child_weight
  )
  
  return(xgb)
}

# Function to do a grid search using a variety of hyperparameters
xgb_grid = function(x, y, etas, max_depths, subsamples) {
  res = tibble(
    eta = numeric(),
    max_depth = numeric(),
    subsample = numeric(),
    test_auc = numeric(),
    model = list(),
    call = list()
  )
  total_iterations = length(etas) * length(max_depths) * length(subsamples)
  pb = progress_bar$new(total = total_iterations,
                         format = "[:bar] :percent ETA: :eta")
  
  for (eta in etas) {
    for (max_depth in max_depths) {
      for (subsample in subsamples) {
        xgb = xgb_options(x, y, eta, max_depth, subsample, verbose = 0)
        test_auc = xgb$evaluation_log[xgb$best_iteration] %>% 
          pull(test_auc_mean)
        res = res %>%
          add_row(eta = eta, 
                  max_depth = max_depth, 
                  subsample = subsample, 
                  test_auc = test_auc,
                  model = list(xgb),
                  call = list(xgb$call)
                  )
        pb$tick()
      }
    }
  }
  res = res %>%
    tibble::rowid_to_column('id')
  
  return(res)
}

# WARNING: this takes a while. A progress bar will show in your console.
seed = 123
xgb_grid_ins = xgb_grid(
  x = ins_t_x, 
  y = ins_t_y, 
  etas = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depths = c(1:10),
  subsamples = c(0.25, 0.5, 0.75, 1),
  seed = seed
)
xgb_grid_ins
```

```{r}
xgb_final_ins_cv = xgb_grid_ins %>%
  arrange(desc(test_auc)) %>%
  head(1)
xgb_final_ins_cv
```

It appears that the best model chosen using cross-validation and without variable selection has the following parameters:
* Learning rate (`eta`): 0.10
* Tree depth (`max_depth`): 4
* Training data fraction (`subsample`): 0.75
* Minimum child weight (`min_child_weight`): 1

> You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.

> Report the variable importance for each of the variables in the model.

```{r}
# training the model using the cv hyperparameters
xgb_final_ins_params = list(
  objective = 'binary:logistic',
  eval_metric = 'auc',
  eta = xgb_final_ins_cv$eta,
  max_depth = xgb_final_ins_cv$max_depth,
  subsample = xgb_final_ins_cv$subsample
)

xgb_ins_dtrain = xgb.DMatrix(
  data = ins_t_x,
  label = ins_t_y
)
xgb_final_ins = xgb.train(
  params = xgb_final_ins_params,
  data = xgb_ins_dtrain,
  nrounds = 100,
  verbose = 1
)

xgb.ggplot.importance(
  xgb.importance(feature_names = colnames(ins_t_x), 
                 model = xgb_final_ins)
)
```


> Report the area under the ROC curve as well as a plot of the ROC curve. (HINT: Use the same approaches you used back in the logistic regression class.)

```{r}

```

