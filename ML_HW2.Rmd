---
title: "Machine Learning HW2"
author: "Fall HW Team 6"
date: "2025-11-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prompt

* For this phase use only the insurance_t data set.
* Previous analysis has identified potential predictor variables related to the purchase of the insurance product so no initial variable selection before model building is necessary.
* The data has missing values that need to be imputed.
  * Typically, the Bank has used median and mode imputation for continuous and categorical variables but are open to other techniques if they are justified in the report.
* The Bank is interested in the value of random forest models.
  * Build a random forest model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. Make sure your target variable is a factor for the random forest.)
  * Tune the model parameters and recommend a final random forest model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
    * Pick one metric to rank things by – no need to report multiple metrics for each variable.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
* The Bank is also interested in the value of an XGBoost model.
  * Build an XGBoost model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. You will need to look up the documentation for the ‘objective = "binary:logistic" ‘ option.)
    * Use the area under the ROC curve (AUC) as your evaluation metric instead of the default in XGBoost.
  * Tune the model parameters and recommend a final XGBoost model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
    
# Setup

## Libraries

```{r}
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(DescTools)
```

## Data

```{r}
ins_t_raw = readr::read_csv('insurance_t.csv')
```

Imputation.

```{r}
ins_t = data.frame(ins_t_raw)
categoricals = c('DDA', 'DIRDEP', 'NSF', 'TELLER', 'SAV',
                 'ATM', 'CD', 'IRA', 'INV', 'MM', 'CC',
                 'SDB', 'INAREA', 'INS', 'BRANCH')

ins_t = ins_t %>%
  mutate(across(categoricals,
                as.factor))

missing = ins_t %>%
  summarize(across(everything(), 
                   ~sum(is.na(.)))) %>%
  t() %>% 
  as.data.frame() %>%
  mutate(n_missing = V1) %>%
  dplyr::select(!V1) %>%
  filter(n_missing > 0)

missing_cols = missing %>% rownames()
# missing columns subsets for numeric and categorical data, respectively
missing_num = setdiff(missing_cols, categoricals)
missing_fctr = intersect(missing_cols, categoricals)

ins_t = ins_t %>%
  # Continuous variables: median imputation
  mutate(
    across(all_of(missing_num),
           ~ replace_na(.x, median(.x, na.rm = T))
    )
  ) %>%
  # Categorical variables: mode imputation
  mutate(
    across(all_of(missing_fctr),
           ~ replace_na(.x, Mode(.x, na.rm = T))
    )
  )
```
j

# Analysis

## Random Forest
```{r}
#Add random variable for feature selection sanity check
ins_t$random <- rnorm(nrow(ins_t))

#Creating Random Forest Model
rfmodel<-randomForest(INS ~ ., data = ins_t)
rfmodel
plot(rfmodel, main = "Random Forest: Error vs Trees")
```


```{r}
var_imp <- varImpPlot(rfmodel)
var_imp
```
#Random variable was 4th highest in importance leading me to believe the model is overfitting




```{r}
#Code tries different mtry(s) for nodesize=5(previous default was nodesize =1)

# Define mtry values
mtry_values <- c(4, 6, 8, 12, 16)

# Train models and store results in a list
set.seed(123)
models <- lapply(mtry_values, function(m) {
  randomForest(INS ~ ., data = ins_t, nodesize =5, mtry = m, ntree = 1000)
})

# Extract OOB error for each model
oob_errors <- sapply(models, function(mod) mod$err.rate[1000, "OOB"])

# Combine results in a single data frame
results <- data.frame(mtry = mtry_values, OOB_Error = oob_errors)
print(results)
```
#OOB error was not much different for any of these 4 mtry values even when increaseing the nodesize

```{r}
library(pROC)

#Convert INS to a factor with valid variable names
ins_t$INS <- factor(ins_t$INS, levels = c(0, 1), labels = c("No", "Yes"))

set.seed(123)
#Set up cross-validation
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary,
                     savePredictions = "final"
)

rfmodelx <- train(
  INS ~ ., data = ins_t,
  method = "rf",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = data.frame(mtry = c(4, 6, 8, 12, 16, 24, 32, 38)),
  ntree = 500
)

rfmodelx
```
```{r}
varImp(rfmodelx)
```

```{r}
library(randomForest)

# Ensure target is binary factor with numeric levels
ins_t$INS <- ins_t_raw$INS
# Convert INS to binary factor with proper labels
ins_t$INS <- factor(as.numeric(as.character(ins_t$INS)), 
                    levels = c(0, 1), 
                    labels = c("No", "Yes"))



set.seed(123)
rf_final <- randomForest(
  INS ~ ., 
  data = ins_t,
  mtry = 16,         # optimal value from caret
  ntree = 500, 
  importance = TRUE  # required to compute permutation importance
)

# Permutation-based variable importance plot
varImpPlot(
  rf_final,
  type = 1, 
  main = "Permutation Importance (Mean Decrease Accuracy)",
  n.var = 15
)

```
#When calculating the variable importance based on Mean Decrease Gini the random variable still ranked highly, however when the metric was changed to calculate mean decrease in accuracy the rankings changed an random did not make it in to the top 15.


```{r}

roc_cv <- roc(
  response = rfmodelx$pred$obs,
  predictor = rfmodelx$pred$Yes,
  levels = c("No", "Yes"),
  direction = "<"
)

auc_value <- auc(roc_cv)
print(paste("Cross-validated AUC:", round(auc_value, 3)))

plot(
  roc_cv,
  col = "#1c61b6",
  lwd = 3,
  main = paste("5-Fold Cross-Validated ROC Curve (AUC =", round(auc_value, 3), ")")
)
abline(a = 0, b = 1, lty = 2, col = "gray")


```







## XGBoost
