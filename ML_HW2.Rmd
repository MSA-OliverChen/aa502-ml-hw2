---
title: "Machine Learning HW2"
author: "Fall HW Team 6"
date: "2025-11-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prompt

* For this phase use only the insurance_t data set.
* Previous analysis has identified potential predictor variables related to the purchase of the insurance product so no initial variable selection before model building is necessary.
* The data has missing values that need to be imputed.
  * Typically, the Bank has used median and mode imputation for continuous and categorical variables but are open to other techniques if they are justified in the report.
* The Bank is interested in the value of random forest models.
  * Build a random forest model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. Make sure your target variable is a factor for the random forest.)
  * Tune the model parameters and recommend a final random forest model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
    * Pick one metric to rank things by – no need to report multiple metrics for each variable.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
* The Bank is also interested in the value of an XGBoost model.
  * Build an XGBoost model.
    * (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. You will need to look up the documentation for the ‘objective = "binary:logistic" ‘ option.)
    * Use the area under the ROC curve (AUC) as your evaluation metric instead of the default in XGBoost.
  * Tune the model parameters and recommend a final XGBoost model.
    * You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.
  * Report the variable importance for each of the variables in the model.
  * Report the area under the ROC curve as well as a plot of the ROC curve.
    * (HINT: Use the same approaches you used back in the logistic regression class.)
    
# Setup

## Libraries

```{r}
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(DescTools)
library(progress)
library(ROCit)
```

## Data

```{r}
ins_t_raw = readr::read_csv('insurance_t.csv')
```

Imputation.

```{r}
ins_t = data.frame(ins_t_raw)
categoricals = c('DDA', 'DIRDEP', 'NSF', 'TELLER', 'SAV',
                 'ATM', 'CD', 'IRA', 'INV', 'MM', 'CC',
                 'SDB', 'INAREA', 'BRANCH')

ins_t = ins_t %>%
  mutate(across(all_of(categoricals),
                as.factor))

missing = ins_t %>%
  summarize(across(everything(), 
                   ~sum(is.na(.)))) %>%
  t() %>% 
  as.data.frame() %>%
  mutate(n_missing = V1) %>%
  dplyr::select(!V1) %>%
  filter(n_missing > 0)

missing_cols = missing %>% rownames()
# missing columns subsets for numeric and categorical data, respectively
missing_num = setdiff(missing_cols, categoricals)
missing_fctr = intersect(missing_cols, categoricals)

ins_t = ins_t %>%
  # Continuous variables: median imputation
  mutate(
    across(all_of(missing_num),
           ~ replace_na(.x, median(.x, na.rm = T))
    )
  ) %>%
  # Categorical variables: mode imputation
  mutate(
    across(all_of(missing_fctr),
           ~ replace_na(.x, Mode(.x, na.rm = T))
    )
  )
```


# Analysis

## Random Forest
```{r}
#Add random variable for feature selection sanity check
ins_t_rf = data.frame(ins_t)
ins_t_rf$random <- rnorm(nrow(ins_t_rf))
ins_t_rf$INS = factor(ins_t_rf$INS, levels = c(0, 1), labels = c("No", "Yes"))

#Creating Random Forest Model
rfmodel<-randomForest(INS ~ ., data = ins_t_rf)
rfmodel
plot(rfmodel, main = "Random Forest: Error vs Trees")
```


```{r}
var_imp <- varImpPlot(rfmodel)
var_imp
```
Random variable was 4th highest in importance leading me to believe the model is overfitting

```{r}
#Code tries different mtry(s) for nodesize=5(previous default was nodesize =1)

# Define mtry values
mtry_values <- c(4, 6, 8, 12, 16)

# Train models and store results in a list
set.seed(123)
models <- lapply(mtry_values, function(m) {
  randomForest(INS ~ ., data = ins_t_rf, nodesize =5, mtry = m, ntree = 1000)
})

# Extract OOB error for each model
oob_errors <- sapply(models, function(mod) mod$err.rate[1000, "OOB"])

# Combine results in a single data frame
results <- data.frame(mtry = mtry_values, OOB_Error = oob_errors)
print(results)
```
OOB error was not much different for any of these 4 mtry values even when increaseing the nodesize

```{r}
library(pROC)

set.seed(123)
#Set up cross-validation
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary,
                     savePredictions = "final"
)

rfmodelx <- train(
  INS ~ ., data = ins_t_rf,
  method = "rf",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = data.frame(mtry = c(4, 6, 8, 12, 16, 24, 32, 38)),
  ntree = 500
)

rfmodelx
```
```{r}
varImp(rfmodelx)
```

```{r}
library(randomForest)

set.seed(123)
rf_final <- randomForest(
  INS ~ ., 
  data = ins_t_rf,
  mtry = 16,         # optimal value from caret
  ntree = 500, 
  importance = TRUE  # required to compute permutation importance
)

# Permutation-based variable importance plot
varImpPlot(
  rf_final,
  type = 1, 
  main = "Permutation Importance (Mean Decrease Accuracy)",
  n.var = 15
)

```
When calculating the variable importance based on Mean Decrease Gini the random variable still ranked highly, however when the metric was changed to calculate mean decrease in accuracy the rankings changed an random did not make it in to the top 15.


```{r}
roc_cv <- roc(
  response = rfmodelx$pred$obs,
  predictor = rfmodelx$pred$Yes,
  levels = c("No", "Yes"),
  direction = "<"
)

auc_value <- auc(roc_cv)
print(paste("Cross-validated AUC:", round(auc_value, 3)))

plot(
  roc_cv,
  col = "#1c61b6",
  lwd = 3,
  main = paste("5-Fold Cross-Validated ROC Curve (AUC =", round(auc_value, 3), ")")
)
abline(a = 0, b = 1, lty = 2, col = "gray")
```

## XGBoost

<!-- 
Resources: 
* https://dhale-2025.github.io/MLBook/Tree_Based_R.html
* https://dhale-2025.github.io/MLBook/rf-xgb-params.html 
-->

> Build an XGBoost model. (HINT: You CANNOT just copy and paste the code from class. In class we built a model to predict a continuous variable. You will need to look up the documentation for the ‘objective = "binary:logistic" ‘ option.)
> Use the area under the ROC curve (AUC) as your evaluation metric instead of the default in XGBoost.

```{r}
ins_t_xgb = data.frame(ins_t)
ins_t_xgb$random = rnorm(nrow(ins_t_xgb))
ins_t_x = model.matrix(INS ~ ., data = ins_t_xgb)[, -1]
ins_t_y = ins_t$INS %>% as.numeric()

set.seed(123)
xgb_ins_1 = xgboost(
  data = ins_t_x,
  label = ins_t_y,
  subsample = 0.5, 
  nrounds = 100,
  # nfold = 5,
  objective = 'binary:logistic', 
  eval_metric = 'auc',
  maximize = T,
  verbose = 0
)
# I think these settings are overfitting
```

These default settings seem to be overfitting. Let's use cross-validation and tweak the hyperparameters (trying a few different values). 

> Tune the model parameters and recommend a final XGBoost model. 

```{r}
# Wrapper function to easily run cross validated
# binary classification XGBoost with AUC as the 
# objective function.
xgb_options = function(x, y, 
                       eta, 
                       max_depth, 
                       subsample, 
                       verbose = 1, 
                       seed = 123,
                       early_stopping_rounds = 10,
                       nfold = 5,
                       nrounds = 100,
                       min_child_weight = 1) {
  set.seed(seed)
  xgb = xgb.cv(
    # settings
    data = x,
    label = y,
    objective = 'binary:logistic', 
    eval_metric = 'auc',
    maximize = T,
    verbose = verbose,
    early_stopping_rounds = early_stopping_rounds,
    
    # hyperparameters
    nfold = nfold,
    eta = eta,
    subsample = subsample, 
    max_depth = max_depth,
    nrounds = nrounds,
    min_child_weight = min_child_weight
  )
  
  return(xgb)
}

# Function to do a grid search using a variety of hyperparameters
xgb_grid = function(x, y, etas, max_depths, subsamples, seed) {
  res = tibble(
    eta = numeric(),
    max_depth = numeric(),
    subsample = numeric(),
    test_auc = numeric(),
    model = list(),
    call = list()
  )
  total_iterations = length(etas) * length(max_depths) * length(subsamples)
  pb = progress_bar$new(total = total_iterations,
                         format = "[:bar] :percent ETA: :eta")
  
  for (eta in etas) {
    for (max_depth in max_depths) {
      for (subsample in subsamples) {
        xgb = xgb_options(x, y, eta, max_depth, subsample, seed = seed, verbose = 0)
        test_auc = xgb$evaluation_log[xgb$best_iteration] %>% 
          pull(test_auc_mean)
        res = res %>%
          add_row(eta = eta, 
                  max_depth = max_depth, 
                  subsample = subsample, 
                  test_auc = test_auc,
                  model = list(xgb),
                  call = list(xgb$call)
                  )
        pb$tick()
      }
    }
  }
  res = res %>%
    tibble::rowid_to_column('id')
  
  return(res)
}

# WARNING: this takes a while. A progress bar will show in your console.
seed = 123
xgb_grid_ins = xgb_grid(
  x = ins_t_x, 
  y = ins_t_y, 
  etas = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depths = c(1:10),
  subsamples = c(0.25, 0.5, 0.75, 1),
  seed = seed
)
xgb_grid_ins
```

```{r}
xgb_final_ins_cv = xgb_grid_ins %>%
  arrange(desc(test_auc)) %>%
  head(1)
xgb_final_ins_cv
```

It appears that the best model chosen using cross-validation and without variable selection has the following parameters:
* Learning rate (`eta`): 0.10
* Tree depth (`max_depth`): 4
* Training data fraction (`subsample`): 0.75
* Minimum child weight (`min_child_weight`): 1

> You are welcome to consider variable selection as well for building your final model. Describe your process for arriving at your final model.

> Report the variable importance for each of the variables in the model.

```{r}
# training the model using the cv hyperparameters
# xgb_ins_final_params = list(
#   objective = 'binary:logistic',
#   eval_metric = 'auc',
#   eta = xgb_final_ins_cv$eta,
#   max_depth = xgb_final_ins_cv$max_depth,
#   subsample = xgb_final_ins_cv$subsample
# )
xgb_ins_final_params = list(
  objective = 'binary:logistic',
  eval_metric = 'auc',
  eta = 0.10,
  max_depth = 4,
  subsample = 0.75
)

xgb_ins_dtrain = xgb.DMatrix(
  data = ins_t_x,
  label = ins_t_y
)
xgb_ins_final_model = xgb.train(
  params = xgb_ins_final_params,
  data = xgb_ins_dtrain,
  nrounds = 100,
  verbose = 1
)

xgb_ins_importance = xgb.importance(
  feature_names = colnames(ins_t_x), 
  model = xgb_ins_final_model
)

xgb.ggplot.importance(xgb_ins_importance)

xgb_ins_importance
```


> Report the area under the ROC curve as well as a plot of the ROC curve. (HINT: Use the same approaches you used back in the logistic regression class.)

```{r}
xgb_ins_phat = predict(xgb_ins_final_model, xgb_ins_dtrain)
xgb_ins_roc = rocit(xgb_ins_phat, ins_t_y)

# AUC
paste('AUC: ', xgb_ins_roc$AUC, sep = '') %>%
  print()

# lift table
xgb_ins_roc %>% gainstable()

cutoff = plot(xgb_ins_roc, YIndex = TRUE, values = TRUE)$optimal[['value']]
title('ROC Curve')
xgb_ins_preds = if_else(xgb_ins_phat >= cutoff, "pred_1", "pred_0")
confusion = table(ins_t$INS, xgb_ins_preds)

accuracy = (confusion[[1, 1]] + confusion[[2, 2]]) / sum(confusion)
paste('Accuracy: ', accuracy, sep = '') %>%
  print()
```

